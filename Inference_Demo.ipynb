{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "from FCN8 import FCN8s\n",
    "from segnet import segnet\n",
    "from createDataset import MyDataset\n",
    "from utils import *\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "# from tensorboardX import SummaryWriter\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "dataroot = 'data/'\n",
    "batch = 16\n",
    "num_classes=39\n",
    "img_size = (128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Transforms ###########\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "input_transforms = transforms.Compose([\n",
    "        transforms.Resize(img_size, interpolation = 1),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "to_tensor =  transforms.Compose([transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Dataloader ###########\n",
    "seg_path = 'gt_labels/'\n",
    "img_path = 'leftImg8bit_orig/'\n",
    "\n",
    "colpath = os.path.join(dataroot, img_path)\n",
    "segpath = os.path.join(dataroot, seg_path)\n",
    "\n",
    "# colimg = os.listdir(colpath)\n",
    "# segimg = os.listdir(segpath)\n",
    "\n",
    "X_train = os.listdir(os.path.join(colpath,'train'))\n",
    "Y_train = os.listdir(os.path.join(segpath,'train'))\n",
    "X_val = os.listdir(os.path.join(colpath,'val'))\n",
    "Y_val = os.listdir(os.path.join(segpath,'val'))\n",
    "X_test = os.listdir(os.path.join(colpath,'test'))\n",
    "Y_test = os.listdir(os.path.join(segpath,'test'))\n",
    "\n",
    "\n",
    "                \n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(colimg, segimg, random_state=123)\n",
    "\n",
    "train_dataset = MyDataset(X_train, Y_train, dataroot, in_transforms = input_transforms, size = img_size,\n",
    "\tphase = 'train')\n",
    "test_dataset = MyDataset(X_test, Y_test, dataroot, in_transforms = input_transforms, size = img_size,\n",
    "\tphase = 'test')\n",
    "val_dataset = MyDataset(X_val, Y_val, dataroot, in_transforms = input_transforms, size = img_size,\n",
    "\tphase = 'val')\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_to_color.txt', 'r') as f:\n",
    "    id_to_color_map = json.load(f)\n",
    "\n",
    "id_to_color_map = {int(key): value for key, value in id_to_color_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertImgToSegColMap(img):\n",
    "    new_img = np.zeros((img.shape[0],img.shape[1],3))\n",
    "#     print(new_img.shape,img.shape)\n",
    "    valid_keys = np.unique(img)\n",
    "    \n",
    "#     new_img[:,:,0],new_img[:,:,1],new_img[:,:,2] = img[:,:,0],img[:,:,0],img[:,:,0]\n",
    "    for key in valid_keys:\n",
    "#         print(key)\n",
    "        x,y = np.where(img==key)\n",
    "#         print(len(x),len(y))\n",
    "        \n",
    "        coords = [list(coord) for coord in zip(x,y)]\n",
    "#         print(len(coords))\n",
    "#         print(max(x),max(y))\n",
    "#         print(id_to_color_map[key])\n",
    "        for coord in coords:\n",
    "\n",
    "#             print(coord)\n",
    "            \n",
    "            new_img[coord[0],coord[1]] = id_to_color_map[key]\n",
    "    return new_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePixelAcc(predictedImg,gt_img):\n",
    "    pred_arr = predictedImg.reshape(-1)\n",
    "    gt_img = gt_img.reshape(-1)\n",
    "    corr_arr = np.zeros_like(pred_arr)\n",
    "    corr_arr[pred_arr==gt_img] = 1\n",
    "    return sum(corr_arr)/pred_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDataImbalance(root,img_dir):\n",
    "    dataPath = os.path.join(root,img_dir)\n",
    "    count_labels = np.zeros((num_classes))\n",
    "    all_labels = os.listdir(dataPath)\n",
    "    for idx,label in enumerate(all_labels):\n",
    "        \n",
    "        gt_lab = cv2.imread(os.path.join(root,img_dir,label),0)\n",
    "        un_labs = np.unique(gt_lab)\n",
    "        count_labels[un_labs]+=1\n",
    "\n",
    "#     print(un_labs)\n",
    "    return count_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tic():\n",
    "    # Homemade version of matlab tic and toc functions\n",
    "    import time\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "def toc():\n",
    "    import time\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print (\"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\")\n",
    "    else:\n",
    "        print (\"Toc: start time not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = checkDataImbalance(segpath,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4966  475 3082 4998 2007 3811    0 2931 1582    0  483 4993 4975 4317\n",
      " 3447  421 2742 4981    2  339   12 5022   77    1  279 2776    7    2\n",
      " 4190 1616 4438  464 2976 4262 4435  197 2230  434    0]\n"
     ]
    }
   ],
   "source": [
    "print(count_labels.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = count_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'out of roi', 1: 'sky', 2: 'animal', 3: 'car', 4: 'bus', 5: 'wall', 6: 'sidewalk', 7: 'tunnel', 8: 'curb', 9: 'train', 10: 'fallback background', 11: 'caravan', 12: 'obs-str-bar-fallback', 13: 'person', 14: 'non-drivable fallback', 15: 'road', 16: 'rectification border', 17: 'billboard', 18: 'drivable fallback', 19: 'license plate', 20: 'traffic light', 21: 'bridge', 22: 'rider', 23: 'bicycle', 24: 'truck', 25: 'building', 26: 'vehicle fallback', 27: 'trailer', 28: 'vegetation', 29: 'autorickshaw', 30: 'fence', 31: 'polegroup', 32: 'rail track', 33: 'motorcycle', 34: 'guard rail', 35: 'traffic sign', 36: 'pole', 37: 'parking', 38: 'ego vehicle'}\n"
     ]
    }
   ],
   "source": [
    "with open('class_id_to_label_map.txt', 'r') as f:\n",
    "    id_to_label_map = json.load(f)\n",
    "    \n",
    "id_to_label_map = {int(key): value for key, value in id_to_label_map.items()}\n",
    "print(id_to_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guard rail': 4435, 'caravan': 4993, 'vehicle fallback': 7, 'road': 421, 'pole': 2230, 'train': 0, 'autorickshaw': 1616, 'building': 2776, 'person': 4317, 'vegetation': 4190, 'tunnel': 2931, 'sky': 475, 'rectification border': 2742, 'curb': 1582, 'non-drivable fallback': 3447, 'traffic sign': 197, 'parking': 434, 'car': 4998, 'billboard': 4981, 'fence': 4438, 'truck': 279, 'out of roi': 4966, 'obs-str-bar-fallback': 4975, 'bridge': 5022, 'bus': 2007, 'sidewalk': 0, 'animal': 3082, 'rail track': 2976, 'ego vehicle': 0, 'polegroup': 464, 'bicycle': 1, 'wall': 3811, 'license plate': 339, 'fallback background': 483, 'drivable fallback': 2, 'rider': 77, 'trailer': 2, 'motorcycle': 4262, 'traffic light': 12}\n"
     ]
    }
   ],
   "source": [
    "occ_dict = {}\n",
    "for idx, occ in enumerate(count_labels):\n",
    "    occ_dict[id_to_label_map[idx]]=occ\n",
    "    \n",
    "print(occ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e79f0bc53e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#             optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#             print(output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Sem5/ML/Project/semantic-segmentation-indian-driving-dataset/FCN8.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mpool3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mpool4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mpool5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saved_models =['weights_fcn.pth','weighted_fcn.pth','weights_segnet_ep8.pth','weights_wt_segnet_sameOP.pth']\n",
    "op_names = ['unweighted_fcn','weighted_fcn','unweighted_segnet','weighted_segnet']\n",
    "root = 'demoSet'\n",
    "gtDir = 'gt_labels'\n",
    "col_Dir = 'gt_labels_colored'\n",
    "orig_Dir = 'leftImg8bit_orig'\n",
    "\n",
    "saved_output_dir = 'saved_outputs'\n",
    "all_imgs = os.listdir(os.path.join(root,orig_Dir))\n",
    "all_test_imgs = os.listdir(os.path.join(colpath,'test'))\n",
    "\n",
    "total_images = len(all_test_imgs)\n",
    "total_iou = 0\n",
    "total_pix_acc = 0\n",
    "# print(id_to_color_map)\n",
    "#### test model accuracy #######\n",
    "for idxx,saved_model in enumerate(saved_models):\n",
    "    if idxx==0 or idx==1:\n",
    "        model = FCN8s(num_classes)\n",
    "    else:  \n",
    "        model = segnet(num_classes)\n",
    "    model.load_state_dict(torch.load(saved_model))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tic()\n",
    "    for idx,img_ in enumerate(all_test_imgs):\n",
    "         if idx==6:\n",
    "            img1 = Image.open(os.path.join(colpath, 'test' ,img_))\n",
    "            img = input_transforms(img1)\n",
    "            img.unsqueeze_(0)\n",
    "            img = img.to(device)\n",
    "    #         print(img.size())\n",
    "\n",
    "    #             optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "\n",
    "    #             print(output.size())\n",
    "            output.squeeze_(0)\n",
    "            output_labels = torch.argmax(output,dim=0)\n",
    "    #             print(output_labels.size())\n",
    "    #             print(output_labels)\n",
    "\n",
    "            np_img = output_labels.detach().cpu().numpy()\n",
    "            np_img = np_img.reshape(np_img.shape[0],np_img.shape[1],1)\n",
    "    #             print(np_img.shape)\n",
    "\n",
    "    #             np.save('ac.npy',np_img)\n",
    "            img1.save(os.path.join('saved_demo_outputs/','orig.png'))\n",
    "            img_col = Image.open(os.path.join(dataroot,col_Dir,'test',img_))\n",
    "            img_col.save(os.path.join('saved_demo_outputs/','col.png'))\n",
    "                         \n",
    "            np_img_res = cv2.resize(np_img,img1.size,interpolation = cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(os.path.join('saved_demo_outputs/','out.png'),np_img_res)\n",
    "\n",
    "            gt_img_name = img_[:img_.find('_')]+str('_id_gt.png')\n",
    "\n",
    "            gt_img = cv2.imread(os.path.join(segpath,'test',gt_img_name),0)\n",
    "\n",
    "            gt_tsor_img = Image.open(os.path.join(segpath,'test',gt_img_name))\n",
    "    #             print(gt_tsor_img.size)\n",
    "    #             gt_tsor_img = gt_tsor_img[:,:,0]\n",
    "    #             gt_tsor_img = Image.fromarray(gt_img)\n",
    "\n",
    "    #             cv2.imwrite('testing.png',gt_img)\n",
    "\n",
    "    #             gt_tsor = input_transforms(gt_tsor_img)\n",
    "    #             gt_tsor = gt_tsor[0,:,:]\n",
    "    #             print('Tsor',gt_tsor.shape)\n",
    "    #             gt_tsor = gt_tsor.long()\n",
    "    #             gt_tsor.unsqueeze_(0)\n",
    "    #             gt_tsor.unsqueeze_(0)\n",
    "    #             gt_tsor = gt_tsor.to(device)\n",
    "    #             print('this',gt_img.shape)\n",
    "    #             print(gt_img[:,:,0]-gt_img[:,:,1])\n",
    "\n",
    "\n",
    "            col_gt_img = convertImgToSegColMap(gt_img)\n",
    "\n",
    "            my_gt_name = str(op_names[idxx])+str('_col_gt.png')\n",
    "            cv2.imwrite(os.path.join('saved_demo_outputs/',my_gt_name),col_gt_img)\n",
    "\n",
    "            col_seg_img = convertImgToSegColMap(np_img_res)\n",
    "            my_seg_name = str(op_names[idxx])+str('_col_seg.png')\n",
    "            cv2.imwrite(os.path.join('saved_demo_outputs/',my_seg_name),col_seg_img)\n",
    "\n",
    "\n",
    "    #             print(np_img_res.shape)\n",
    "    #             print(np.array(gt_img).shape)\n",
    "    #             gt_img = np.array(gt_img)[:,:,0]\n",
    "    #             print(gt_img)\n",
    "    #             print(gt_img.shape)\n",
    "    #             print(gt_img.reshape(-1).shape)\n",
    "            pix_acc = calculatePixelAcc(np_img_res,gt_img)\n",
    "            output = output.unsqueeze_(0)\n",
    "    #         print(output.size())\n",
    "    #             test = torch.max(output.data, 1)[1]\n",
    "    #             test = test.long()\n",
    "    #             test = test.to(device)\n",
    "\n",
    "            out_labels = torch.max(output.data, 1)[1]\n",
    "    #             out_labels.unsqueeze_(0)\n",
    "            out_labels_np = out_labels.cpu().numpy().transpose((1,2,0))\n",
    "\n",
    "    #             print('outlabels',out_labels_np.shape)\n",
    "    #             print('gt_tsor',gt_tsor.size())\n",
    "    #             print(gt_tsor)\n",
    "\n",
    "            gt_res_img = cv2.resize(gt_img,(128,128),interpolation = cv2.INTER_NEAREST)\n",
    "            gt_res_img = gt_res_img.reshape(gt_res_img.shape[0],gt_res_img.shape[1],1)\n",
    "    #             print('gt_img_res',gt_res_img.shape)\n",
    "            intersection = np.bitwise_and(out_labels_np,gt_res_img)\n",
    "            union = np.bitwise_or(out_labels_np,gt_res_img)\n",
    "    #             print(intersection,union)\n",
    "            iou = np.mean(np.sum(intersection)/np.sum(union))\n",
    "            print('iou',iou)\n",
    "    #             iou = torch.mean((torch.sum(intersection).float()/torch.sum(union).float()).float())\n",
    "    #             print('iou = ',checkiou(out_labels,gt_tsor,1).item())\n",
    "            total_iou+=iou\n",
    "            total_pix_acc+=pix_acc\n",
    "    toc()\n",
    "#     print(\"Mean IOU = \",total_iou/total_images)\n",
    "#     print(\"Total Pix Acc = \",total_pix_acc/total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2380352644836272"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "378/total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
